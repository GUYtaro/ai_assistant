# core/live_vision_stream.py
# ================================
# üé• Live Vision Stream System
# ‡πÅ‡∏ä‡∏£‡πå‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÅ‡∏ö‡∏ö Real-time + AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Gemini Live)
# ================================

import cv2
import numpy as np
import time
from threading import Thread, Event
from queue import Queue
from PyQt6.QtCore import QObject, pyqtSignal, QTimer
from core.screen_capturer import screenshot_pil
from core.llm_client import LLMClient
from datetime import datetime


class LiveVisionStream(QObject):
    """
    üé• Live Vision Stream - ‡πÅ‡∏ä‡∏£‡πå‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÅ‡∏ö‡∏ö real-time
    
    ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå:
    - ‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (10-15 FPS)
    - ‡πÅ‡∏™‡∏î‡∏á preview window ‡πÅ‡∏ö‡∏ö real-time
    - AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å N ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡πÑ‡∏°‡πà‡∏ö‡∏•‡πá‡∏≠‡∏Å stream)
    - ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡∏ì‡∏∞ streaming
    """
    
    # ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ï‡πà‡∏≤‡∏á‡πÜ
    frame_captured = pyqtSignal(object)      # ‡∏™‡πà‡∏á‡πÄ‡∏ü‡∏£‡∏°‡∏†‡∏≤‡∏û
    analysis_ready = pyqtSignal(str)         # ‡∏™‡πà‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    status_updated = pyqtSignal(str)         # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
    stream_started = pyqtSignal()            # ‡πÄ‡∏£‡∏¥‡πà‡∏° stream
    stream_stopped = pyqtSignal()            # ‡∏´‡∏¢‡∏∏‡∏î stream
    
    def __init__(self, llm_client: LLMClient = None, monitor=1):
        super().__init__()
        self.llm = llm_client or LLMClient()
        self.monitor = monitor
        
        # Stream settings
        self.is_streaming = False
        self.fps = 10  # 10 ‡πÄ‡∏ü‡∏£‡∏°‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
        self.frame_interval = 1.0 / self.fps
        
        # AI analysis settings
        self.analysis_interval = 3.0  # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å 3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        self.last_analysis_time = 0
        self.auto_analysis = True  # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        
        # Threading
        self.stream_thread = None
        self.analysis_thread = None
        self.stop_event = Event()
        
        # Frame queue ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI analysis
        self.frame_queue = Queue(maxsize=2)
        
        # Latest frame ‡πÅ‡∏•‡∏∞ analysis
        self.latest_frame = None
        self.latest_analysis = ""
        
        # Statistics
        self.frame_count = 0
        self.analysis_count = 0
        self.start_time = None
        
        print("[LiveVision] ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏£‡∏∞‡∏ö‡∏ö Live Vision Stream")
    
    def start_stream(self, fps=10, analysis_interval=3.0):
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏° Live Vision Stream
        
        Parameters:
            fps: ‡πÄ‡∏ü‡∏£‡∏°‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ 5-15)
            analysis_interval: ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        """
        if self.is_streaming:
            print("[LiveVision] ‚ö†Ô∏è Stream ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß")
            return
        
        self.is_streaming = True
        self.fps = fps
        self.frame_interval = 1.0 / fps
        self.analysis_interval = analysis_interval
        self.stop_event.clear()
        
        self.frame_count = 0
        self.analysis_count = 0
        self.start_time = time.time()
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏° capture thread
        self.stream_thread = Thread(target=self._capture_loop, daemon=True)
        self.stream_thread.start()
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏° analysis thread
        if self.auto_analysis:
            self.analysis_thread = Thread(target=self._analysis_loop, daemon=True)
            self.analysis_thread.start()
        
        self.stream_started.emit()
        self.status_updated.emit(f"üé• Live Stream ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏•‡πâ‡∏ß ({fps} FPS)")
        print(f"[LiveVision] üü¢ ‡πÄ‡∏£‡∏¥‡πà‡∏° Stream @ {fps} FPS, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏∏‡∏Å {analysis_interval}s")
    
    def stop_stream(self):
        """‡∏´‡∏¢‡∏∏‡∏î Live Vision Stream"""
        if not self.is_streaming:
            return
        
        self.is_streaming = False
        self.stop_event.set()
        
        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ threads ‡∏à‡∏ö
        if self.stream_thread:
            self.stream_thread.join(timeout=2.0)
        if self.analysis_thread:
            self.analysis_thread.join(timeout=2.0)
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        duration = time.time() - self.start_time if self.start_time else 0
        avg_fps = self.frame_count / duration if duration > 0 else 0
        
        self.stream_stopped.emit()
        self.status_updated.emit(f"‚è∏Ô∏è ‡∏´‡∏¢‡∏∏‡∏î Stream (‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ {self.frame_count} ‡πÄ‡∏ü‡∏£‡∏°, {avg_fps:.1f} FPS)")
        print(f"[LiveVision] üî¥ ‡∏´‡∏¢‡∏∏‡∏î Stream")
        print(f"  üìä ‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {self.frame_count}")
        print(f"  üìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {self.analysis_count} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        print(f"  üìä FPS ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_fps:.1f}")
    
    def _capture_loop(self):
        """
        Loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á
        ‡∏£‡∏±‡∏ô‡πÉ‡∏ô background thread
        """
        print("[LiveVision] üé• ‡πÄ‡∏£‡∏¥‡πà‡∏° Capture Loop")
        
        while not self.stop_event.is_set():
            try:
                frame_start = time.time()
                
                # ‡∏à‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
                img = screenshot_pil(monitor=self.monitor)
                
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô numpy array (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö OpenCV)
                frame = np.array(img)
                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                
                # ‡∏¢‡πà‡∏≠‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
                height, width = frame.shape[:2]
                if width > 1280:
                    scale = 1280 / width
                    new_width = 1280
                    new_height = int(height * scale)
                    frame = cv2.resize(frame, (new_width, new_height))
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° overlay ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
                self._add_overlay_info(frame)
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
                self.latest_frame = frame.copy()
                self.frame_count += 1
                
                # ‡∏™‡πà‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
                self.frame_captured.emit(frame)
                
                # ‡πÉ‡∏™‡πà‡πÄ‡∏Ç‡πâ‡∏≤ queue ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI analysis (‡∏ñ‡πâ‡∏≤‡∏ß‡πà‡∏≤‡∏á)
                if self.auto_analysis and not self.frame_queue.full():
                    try:
                        self.frame_queue.put_nowait(img)
                    except:
                        pass
                
                # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ü‡∏£‡∏°
                elapsed = time.time() - frame_start
                sleep_time = max(0, self.frame_interval - elapsed)
                if sleep_time > 0:
                    time.sleep(sleep_time)
                
            except Exception as e:
                print(f"[LiveVision] ‚ùå Capture Error: {e}")
                time.sleep(0.1)
        
        print("[LiveVision] üõë Capture Loop ‡∏´‡∏¢‡∏∏‡∏î")
    
    def _analysis_loop(self):
        """
        Loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ AI
        ‡∏£‡∏±‡∏ô‡πÉ‡∏ô background thread ‡πÅ‡∏¢‡∏Å‡∏à‡∏≤‡∏Å capture
        """
        print("[LiveVision] ü§ñ ‡πÄ‡∏£‡∏¥‡πà‡∏° Analysis Loop")
        
        while not self.stop_event.is_set():
            try:
                current_time = time.time()
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á
                if current_time - self.last_analysis_time < self.analysis_interval:
                    time.sleep(0.5)
                    continue
                
                # ‡∏î‡∏∂‡∏á‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏≤‡∏Å queue
                if self.frame_queue.empty():
                    time.sleep(0.5)
                    continue
                
                img = self.frame_queue.get()
                
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI
                self._analyze_frame(img)
                
                self.last_analysis_time = current_time
                
            except Exception as e:
                print(f"[LiveVision] ‚ùå Analysis Error: {e}")
                time.sleep(1.0)
        
        print("[LiveVision] üõë Analysis Loop ‡∏´‡∏¢‡∏∏‡∏î")
    
    def _analyze_frame(self, img):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ü‡∏£‡∏°‡∏î‡πâ‡∏ß‡∏¢ AI"""
        try:
            self.analysis_count += 1
            print(f"[LiveVision] ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {self.analysis_count})...")
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô data URI
            from core.screen_capturer import image_to_data_uri
            data_uri, _ = image_to_data_uri(img, fmt="JPEG", quality=70)
            
            # ‡∏ñ‡∏≤‡∏° AI
            prompt = "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100 ‡∏Ñ‡∏≥)"
            analysis = self.llm.ask_with_image(prompt, data_uri)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.latest_analysis = analysis
            
            # ‡∏™‡πà‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì
            timestamp = datetime.now().strftime("%H:%M:%S")
            result = f"[{timestamp}] {analysis}"
            self.analysis_ready.emit(result)
            
            print(f"[LiveVision] ‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à: {analysis[:50]}...")
            
        except Exception as e:
            print(f"[LiveVision] ‚ùå AI Error: {e}")
    
    def _add_overlay_info(self, frame):
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• overlay ‡∏ö‡∏ô‡πÄ‡∏ü‡∏£‡∏°"""
        # ‡∏ß‡∏≤‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 100), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
        
        # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
        duration = time.time() - self.start_time if self.start_time else 0
        avg_fps = self.frame_count / duration if duration > 0 else 0
        
        # ‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        y = 30
        cv2.putText(frame, f"üé• Live Vision Stream", (15, y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        y += 25
        cv2.putText(frame, f"FPS: {avg_fps:.1f} | Frames: {self.frame_count}", (15, y),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        y += 20
        cv2.putText(frame, f"AI Analysis: {self.analysis_count} times", (15, y),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def ask_about_screen(self, question: str):
        """
        ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        ‡πÉ‡∏ä‡πâ‡πÄ‡∏ü‡∏£‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ
        """
        if self.latest_frame is None:
            return "‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠"
        
        try:
            print(f"[LiveVision] üí¨ ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}")
            
            # ‡πÅ‡∏õ‡∏•‡∏á frame ‡πÄ‡∏õ‡πá‡∏ô PIL Image
            from PIL import Image
            frame_rgb = cv2.cvtColor(self.latest_frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô data URI
            from core.screen_capturer import image_to_data_uri
            data_uri, _ = image_to_data_uri(img, fmt="JPEG", quality=80)
            
            # ‡∏ñ‡∏≤‡∏° AI
            answer = self.llm.ask_with_image(question, data_uri)
            
            print(f"[LiveVision] ü§ñ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {answer[:100]}...")
            return answer
            
        except Exception as e:
            print(f"[LiveVision] ‚ùå Ask Error: {e}")
            return f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"
    
    def set_auto_analysis(self, enabled: bool):
        """‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥"""
        self.auto_analysis = enabled
        status = "‡πÄ‡∏õ‡∏¥‡∏î" if enabled else "‡∏õ‡∏¥‡∏î"
        print(f"[LiveVision] ‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥: {status}")
    
    def set_fps(self, fps: int):
        """‡∏õ‡∏£‡∏±‡∏ö FPS ‡∏Ç‡∏ì‡∏∞ streaming"""
        self.fps = max(1, min(fps, 30))  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 1-30 FPS
        self.frame_interval = 1.0 / self.fps
        print(f"[LiveVision] ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á FPS: {self.fps}")
    
    def set_analysis_interval(self, seconds: float):
        """‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
        self.analysis_interval = max(1.0, seconds)
        print(f"[LiveVision] ‚öôÔ∏è ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: {self.analysis_interval}s")
    
    def get_stats(self) -> dict:
        """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
        duration = time.time() - self.start_time if self.start_time else 0
        avg_fps = self.frame_count / duration if duration > 0 else 0
        
        return {
            "is_streaming": self.is_streaming,
            "frame_count": self.frame_count,
            "analysis_count": self.analysis_count,
            "duration": duration,
            "avg_fps": avg_fps,
            "target_fps": self.fps,
            "analysis_interval": self.analysis_interval
        }


# ‚úÖ Test Mode
if __name__ == "__main__":
    import sys
    from PyQt6.QtWidgets import QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout
    from PyQt6.QtWidgets import QPushButton, QLabel, QTextEdit, QLineEdit
    from PyQt6.QtGui import QImage, QPixmap
    
    class LiveVisionTestWindow(QMainWindow):
        """‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏î‡∏™‡∏≠‡∏ö Live Vision Stream"""
        
        def __init__(self):
            super().__init__()
            self.setWindowTitle("üé• Live Vision Stream Test")
            self.setGeometry(100, 100, 1000, 700)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö
            self.live_vision = LiveVisionStream()
            self.live_vision.frame_captured.connect(self.on_frame)
            self.live_vision.analysis_ready.connect(self.on_analysis)
            self.live_vision.status_updated.connect(self.on_status)
            
            self.setup_ui()
        
        def setup_ui(self):
            central = QWidget()
            self.setCentralWidget(central)
            layout = QVBoxLayout(central)
            
            # ‡πÅ‡∏ñ‡∏ö‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°
            control_layout = QHBoxLayout()
            
            self.start_btn = QPushButton("üü¢ Start Stream")
            self.start_btn.clicked.connect(self.start_stream)
            
            self.stop_btn = QPushButton("üî¥ Stop Stream")
            self.stop_btn.clicked.connect(self.stop_stream)
            self.stop_btn.setEnabled(False)
            
            self.status_label = QLabel("‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            
            control_layout.addWidget(self.start_btn)
            control_layout.addWidget(self.stop_btn)
            control_layout.addWidget(self.status_label)
            control_layout.addStretch()
            
            layout.addLayout(control_layout)
            
            # Preview video
            self.video_label = QLabel("üì∏ Video Preview")
            self.video_label.setStyleSheet("border: 2px solid #555; background: #000;")
            self.video_label.setFixedSize(800, 450)
            layout.addWidget(self.video_label)
            
            # ‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            question_layout = QHBoxLayout()
            self.question_input = QLineEdit()
            self.question_input.setPlaceholderText("‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠...")
            self.ask_btn = QPushButton("ü§î ‡∏ñ‡∏≤‡∏°")
            self.ask_btn.clicked.connect(self.ask_question)
            
            question_layout.addWidget(self.question_input)
            question_layout.addWidget(self.ask_btn)
            layout.addLayout(question_layout)
            
            # Analysis output
            self.analysis_text = QTextEdit()
            self.analysis_text.setReadOnly(True)
            self.analysis_text.setMaximumHeight(150)
            layout.addWidget(self.analysis_text)
        
        def on_frame(self, frame):
            """‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏ü‡∏£‡∏°‡∏ö‡∏ô video preview"""
            height, width, channel = frame.shape
            bytes_per_line = 3 * width
            q_image = QImage(frame.data, width, height, bytes_per_line, QImage.Format.Format_BGR888)
            
            # ‡∏¢‡πà‡∏≠‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô label
            pixmap = QPixmap.fromImage(q_image)
            scaled = pixmap.scaled(800, 450, aspectRatioMode=1)
            self.video_label.setPixmap(scaled)
        
        def on_analysis(self, text):
            """‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
            self.analysis_text.append(f"\n{text}\n{'-'*50}")
        
        def on_status(self, text):
            """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞"""
            self.status_label.setText(text)
        
        def start_stream(self):
            self.live_vision.start_stream(fps=10, analysis_interval=5.0)
            self.start_btn.setEnabled(False)
            self.stop_btn.setEnabled(True)
        
        def stop_stream(self):
            self.live_vision.stop_stream()
            self.start_btn.setEnabled(True)
            self.stop_btn.setEnabled(False)
        
        def ask_question(self):
            question = self.question_input.text().strip()
            if not question:
                return
            
            self.question_input.clear()
            self.analysis_text.append(f"\nüí¨ ‡∏Ñ‡∏∏‡∏ì: {question}")
            
            answer = self.live_vision.ask_about_screen(question)
            self.analysis_text.append(f"ü§ñ AI: {answer}\n{'-'*50}")
    
    app = QApplication(sys.argv)
    window = LiveVisionTestWindow()
    window.show()
    
    print("üß™ [Test] Live Vision Stream")
    print("üìç ‡∏Å‡∏î Start Stream ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ä‡∏£‡πå‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠")
    print("üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ñ‡∏≤‡∏° AI ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠")
    
    sys.exit(app.exec())